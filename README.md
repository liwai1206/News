# News
神经网络与深度学习课程设计（Neural_NetworksAndDeep_Learning新闻文本分类）
RNN、CNN网络模型分别实现的新闻文本分类

选题自2021年中软杯——新闻文本分类

选题要求：对新闻进行科学的分类既能够方便不同的阅读群体，根据需求快速选取自身感兴趣的新闻，也能够有效满足对海量的新闻素材提供科学的检索需求

RNN实现步骤：
1. 预处理：在RNN网络中，我们首先需要读取数据，由于每一条新闻的长度并不都是相同的，所以首先要将所有句子都规定成相同的长度。这里的句子长度并不是指所具有的字数相同，而是根据自己的分词算法，一句话分词结果的词的个数相同。对于词数不够的采用特殊码代替，我用的是PAD（词表中的特殊码，下面会介绍）； 超过的部分进行截断丢弃。这里句子长度的选取需要根据自己的数据集的长度来选取，在我的数据集中，大多数新闻都是32词左右，故我选取的长度是32
2. 分词：读取新闻数据的一行，以tab 分隔标题和标签，读取vocab.pkl 文件，获取词表，如图:共有 4761个词，其中这里的PAD就是上面说到的特殊码，用来补充长度不够的句子。
3. 转换： RNN模型中，接收的输入是向量而不是文本，所以需要将一句话中的所有词都转变成向量，也就是进行词嵌入embedding。我这里直接采用了搜狗已经训练好的词嵌入模型embedding_SougouNews.npz
需要说明的是，因为新闻文本基本上都是一些常见的词，所以可以使用已经训练好的词嵌入模型，如果是做一些专业领域的词向量embedding，就需要自己训练模型。在搜狗的词嵌入模型中，官方论文给出的维度是300维，所以我在程序中也是指定300维的向量。
4. 输入维度： RNN网络中的输入维度 ( batch, max-word, f )其中 batch 指的是一次同时处理的新闻数量。例如，我在程序中指定的是 batch = 128， 即一次处理128条新闻。这里的max-word就是上面说到过的 一句话的最大长度 32， 也就是每句话进行embedding之前都具有32个词。而 f 就是每个词的维度，即上面规定的 300维

RNN分层：
第一层： 输入层，即embedding层
第二层： lstm层。参数（300维，隐藏神经元， lstm的层数，双向，第一维度是batch，dropout）。
第三层：全连接层。 参数（ 特征数， 分类数）
第四层： 输出层。

LSTM训练
Lstm的训练主要是循环所有epoch中的所有batch，epoch是指定的，这里为10。
循环batch的过程中，与上面分析的一样，首先正向传播获取特征，然后反向传播获取特征，最后做拼接后输出。
每100轮做一次验证，根据batch中所有新闻的标签获取新闻分类的真实值，将输出中新闻对应的分类概率最大的那个作为预测值，计算出准确率和损失。如果这一次的损失比之前的更小，说明这一次训练得到的模型更好，故保存新的模型。然后循环下一个batch，循环往复
